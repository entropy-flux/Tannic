\chapter{Contributing}
\hypertarget{md_docs_2CONTRIBUTING}{}\label{md_docs_2CONTRIBUTING}\index{Contributing@{Contributing}}
\label{md_docs_2CONTRIBUTING_autotoc_md73}%
\Hypertarget{md_docs_2CONTRIBUTING_autotoc_md73}%
 Contributions to Tannic are welcome! Whether you’re reporting bugs, proposing features, improving documentation, or optimizing kernels, your help is greatly appreciated.\hypertarget{md_docs_2CONTRIBUTING_autotoc_md74}{}\doxysection{\texorpdfstring{Ways to Contribute}{Ways to Contribute}}\label{md_docs_2CONTRIBUTING_autotoc_md74}

\begin{DoxyItemize}
\item {\bfseries{Write new test cases to cover code that is not yet tested}}. I’m using a test-\/driven approach to build the library, but some edge cases may still be missing.
\item {\bfseries{Refactor existing tests to use built-\/in features}}. Many current tests manipulate tensor pointers directly to verify values because they were written before proper indexing and slicing functionality was implemented. This approach is tedious and can be simplified using tensor accessors. Take as example\+:
\end{DoxyItemize}


\begin{DoxyCode}{0}
\DoxyCodeLine{Tensor\ x(float32,\ \{2,2\});\ x.initialize()}
\DoxyCodeLine{\textcolor{keywordtype}{float}*\ data\ =\ \textcolor{keyword}{reinterpret\_cast<}\textcolor{keywordtype}{float}*\textcolor{keyword}{>}(x.bytes());}
\DoxyCodeLine{data[3]\ =\ 3;}
\DoxyCodeLine{ASSERT\_EQ(data[3],\ 3);}

\end{DoxyCode}


can be refactored into\+:


\begin{DoxyCode}{0}
\DoxyCodeLine{Tensor\ x(float32,\ \{2,2\});\ x.initialize()\ }
\DoxyCodeLine{x[1][1]\ =\ 3;}
\DoxyCodeLine{ASSERT\_EQ(x[1][1],\ 3);\ \textcolor{comment}{//\ GTest\ support\ this\ but\ not\ \ ASSERT\_EQ(x[1,1],\ 3)}}

\end{DoxyCode}


This is especially important in CUDA tests, where manually copying device memory sync to the host hurts test performance.


\begin{DoxyItemize}
\item {\bfseries{Improve installation documentation}}. One of the main challenges with C++ adoption is the complexity of building and linking libraries. I plan to create a comprehensive guide on installing and integrating the library into other projects.
\item {\bfseries{Optimize builds}}. Currently there is a single CMake\+Lists.\+txt inside the cmake folder that compiles all the project. Decoupled builds for cpu and cuda backends will be a nice to have.
\item {\bfseries{Optimize kernels}}. Kernels are currently unoptimized since I\textquotesingle{}m still focusing on builing necessary features. The kernels can be found on .cpp and .cu files inside src/cpu and src/cuda files.
\item {\bfseries{New features}}. If you propose new features for the library, please ensure they align with the scope of a tensor library. For example, operations like tensor contractions would be a great addition, but machine learning components —such as neural network activation functions or attention mechanisms— are outside the scope. (Don’t worry—I\textquotesingle{}m building a separate neural networks library on top of this one!) That said, I’m always open to fresh ideas, so don’t hesitate to share your suggestions.
\end{DoxyItemize}\hypertarget{md_docs_2CONTRIBUTING_autotoc_md75}{}\doxysection{\texorpdfstring{How to Contribute}{How to Contribute}}\label{md_docs_2CONTRIBUTING_autotoc_md75}
Fork the repository and create a new branch for your feature or bug fix. Example\+:


\begin{DoxyCode}{0}
\DoxyCodeLine{git\ checkout\ -\/b\ metal/metal-\/backend}

\end{DoxyCode}


Open a pull request describing\+:
\begin{DoxyItemize}
\item The purpose of your changes.
\item Any relevant issues they address.
\item Implementation details if needed.
\end{DoxyItemize}

Target branch\+: PRs for now should just target main till the library matures.\hypertarget{md_docs_2CONTRIBUTING_autotoc_md76}{}\doxysection{\texorpdfstring{Project structure}{Project structure}}\label{md_docs_2CONTRIBUTING_autotoc_md76}
The project is organized into the following main components\+:


\begin{DoxyItemize}
\item Frontend (C++23) – Implemented in C++23 and distributed across multiple .hpp header files in include/tannic/.
\begin{DoxyItemize}
\item Implementations of non-\/constexpr and non templated functions are located in the src/ directory.
\item Some functions currently implemented in headers but not marked constexpr (e.\+g., certain member functions of the {\ttfamily Tensor} class) may become constexpr in the future.
\end{DoxyItemize}
\item Backends – Contain platform-\/specific execution code\+:
\begin{DoxyItemize}
\item src/cpu/ for the CPU backend.
\item src/cuda/ for the CUDA backend.
\end{DoxyItemize}
\item C Runtime Utilities – C utilities located in include/tannic/runtime/, used for building the C API required to extend the library, writting the backend and binding it to the C++ frontend.
\begin{DoxyItemize}
\item All kernels must be implemented in terms of the C interfaces.
\item Vendor-\/specific constructs (e.\+g., streams, events, handles) must not be exposed in the C API. Instead, they should be abstracted using IDs or type erasure.
\begin{DoxyItemize}
\item Example\+: {\ttfamily cuda\+Stream\+\_\+t} represents a computation queue, which is not specific to CUDA. In the C API, it is stored in a type-\/erased form\+:
\end{DoxyItemize}

{\ttfamily c struct stream\+\_\+t \{ uintptr\+\_\+t address; \}; \texorpdfstring{$<$}{<}br\texorpdfstring{$>$}{>} }

Then if it was created as a cuda stream will be recovered as\+:

{\ttfamily cpp cuda\+Stream\+\_\+t cuda\+Stream = reinterpret\+\_\+cast\texorpdfstring{$<$}{<}cuda\+Stream\+\_\+t\texorpdfstring{$>$}{>}(stream.\+address); }
\item C utilities should not be exposed in the C++ frontend, except for data types (dtypes) which are included for compatibility and convenience.
\end{DoxyItemize}
\end{DoxyItemize}

Currently what is in the src root folder is a mess, lot of code repetition and nasty helper functions with bunch of templates. I promise I will take the time to refactor this but after I find a way to dynamically add and remove nodes from a graph dynamically based on reference counting. This refactor won\textquotesingle{}t change anything on the public API.\hypertarget{md_docs_2CONTRIBUTING_autotoc_md77}{}\doxysection{\texorpdfstring{Creating new features.}{Creating new features.}}\label{md_docs_2CONTRIBUTING_autotoc_md77}
The C++ frontend is based on templated expressions, this means that when you write an expression, for example\+:


\begin{DoxyCode}{0}
\DoxyCodeLine{\textcolor{keyword}{auto}\ expr\ =\ \mbox{\hyperlink{namespacetannic_a3681656aa208b88dfc79a1806cf669b6}{matmul}}(X,\ Y)\ +\ Z;}

\end{DoxyCode}


The result is not computed inmediatly, instead a templated expression is created with type\+:


\begin{DoxyCode}{0}
\DoxyCodeLine{Binary<Addition,\ Transformation<Composition,\ Tensor,\ Tensor>>,\ Tensor>\ //\ (matmul\ is\ actually\ a\ composition\ of\ tensors\ :)}

\end{DoxyCode}


This expression holds the the following methods\+:


\begin{DoxyItemize}
\item constexpr type dtype()\+: The resulting dtype of the expression, calculated using promotion tables or custom logic.
\item constexpr Shape shape()\+: The resulting broadcasted shape of the expression.
\item constexpr Strides strides()\+: The resulting strides of the expression, calculated from the shape.
\item ptrdiff\+\_\+t offset()\+: The possition in bytes where the tensor starts in the current buffer. In this case 0 since a new tensor is created.
\item Tensor forward()\+: A method that actually performs the computation using the already calculated metadata.
\end{DoxyItemize}

This allows you to\+:


\begin{DoxyItemize}
\item Create new symbols\+: All expressions that follows the concept (don\textquotesingle{}t worry this is just like a python protocol)\+:


\begin{DoxyCode}{0}
\DoxyCodeLine{\textcolor{keyword}{template}<\textcolor{keyword}{typename}\ T>}
\DoxyCodeLine{\textcolor{keyword}{concept\ }Expression\ =\ \textcolor{keyword}{requires}(\textcolor{keyword}{const}\ T\ expression)\ \{}
\DoxyCodeLine{\ \ \ \ \{\ expression.dtype()\ \ \ \}\ -\/>\ std::same\_as<type>;}
\DoxyCodeLine{\ \ \ \ \{\ expression.shape()\ \ \ \}\ -\/>\ std::same\_as<Shape\ const\&>;}
\DoxyCodeLine{\ \ \ \ \{\ expression.strides()\ \}\ -\/>\ std::same\_as<Strides\ const\&>;}
\DoxyCodeLine{\ \ \ \ expression.offset();\ \ \textcolor{comment}{//\ convertible\ to\ ptrdifft\_t}}
\DoxyCodeLine{\ \ \ \ expression.forward();\ \textcolor{comment}{//\ same\ as\ Tensor\ or\ Tensor\ const\&}}
\DoxyCodeLine{\};\ }

\end{DoxyCode}


Will work with current {\ttfamily Tensor} class and other templated expressions in this library.
\item Create new data structures\+: Again if your data structure follows the prior concept it can be used as well with the library, for example you can create {\ttfamily Scalar}, {\ttfamily Parameter}, {\ttfamily Sequence} classes and plug them into operations like if they were tensors, the resulting expression will be something like this\+:
\end{DoxyItemize}


\begin{DoxyCode}{0}
\DoxyCodeLine{Binary<Addition,\ Transformation<Composition,\ Parameter,\ Sequence>>,\ Scalar>}

\end{DoxyCode}


Finally the computation will be done when calling forward or when the expression is assigned to a non const tensor\+:


\begin{DoxyCode}{0}
\DoxyCodeLine{Tensor\ W\ =\ \mbox{\hyperlink{namespacetannic_a3681656aa208b88dfc79a1806cf669b6}{matmul}}(X,\ Y)\ +\ Z;}

\end{DoxyCode}


This allows a python alike behavior since you can chain operations on the same variable like this\+:


\begin{DoxyCode}{0}
\DoxyCodeLine{Tensor\ W\ =\ \mbox{\hyperlink{namespacetannic_a3681656aa208b88dfc79a1806cf669b6}{matmul}}(X,\ Y)\ +\ Z;}
\DoxyCodeLine{W\ =\ W\ *\ \mbox{\hyperlink{namespacetannic_1_1function_a9ec44a47e78a5e28b75b76573d98e06b}{log}}(Z);}
\DoxyCodeLine{W\ =\ W\ *\ \mbox{\hyperlink{namespacetannic_1_1function_a1a8e818c37a5236e4481677e70ce6209}{exp}}(Y)\ +\ X[1];\ }
\DoxyCodeLine{std::cout\ <<\ W[1]\ +\ Z[0];\ \textcolor{comment}{//\ IO\ supported\ for\ expressions.}}

\end{DoxyCode}
 