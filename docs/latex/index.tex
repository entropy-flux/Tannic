\chapter{Tannic}
\hypertarget{index}{}\label{index}\index{Tannic@{Tannic}}
\label{index_md_docs_2INDEX}%
\Hypertarget{index_md_docs_2INDEX}%
 {\bfseries{Tannic}} is an extensible C++ tensor library built around a host–device execution model. Unlike monolithic frameworks, it provides only a minimal set of built‑in operators, focusing on a flexible architecture where new operations, data types, and backends can be added easily. This approach keeps the library lightweight while enabling adaptation to a wide range of computational needs.

This library is designed to serve as the foundational core for a neural network inference framework, but is equally suited to other domains such as classical ML or physics simulations—all without requiring Python.

Below is a minimal example demonstrating tensor creation, initialization, basic indexing, and arithmetic operations with Tannic\+:


\begin{DoxyCode}{0}
\DoxyCodeLine{\textcolor{preprocessor}{\#include\ <iostream>}}
\DoxyCodeLine{\textcolor{preprocessor}{\#include\ <tannic.hpp>}}
\DoxyCodeLine{}
\DoxyCodeLine{\textcolor{keyword}{using\ namespace\ }\mbox{\hyperlink{namespacetannic}{tannic}};}
\DoxyCodeLine{}
\DoxyCodeLine{\textcolor{keywordtype}{int}\ main()\ \{\ }
\DoxyCodeLine{\ \ \ \ \mbox{\hyperlink{classtannic_1_1Tensor}{Tensor}}\ X(float32,\ \{2,2\});\ X.\mbox{\hyperlink{classtannic_1_1Tensor_a4ee063da4965ea868081841b3d967882}{initialize}}();\ \textcolor{comment}{//\ or\ X.initialize(Device())\ for\ CUDA\ support}}
\DoxyCodeLine{\ \ \ \ }
\DoxyCodeLine{\ \ \ \ X[0,\ \mbox{\hyperlink{structtannic_1_1indexing_1_1Range}{range}}\{0,-\/1\}]\ =\ 1;\ \ }
\DoxyCodeLine{\ \ \ \ X[1,0]\ =\ 3;\ \ \ \ \ \ \ \ \ \ \ \ \ }
\DoxyCodeLine{\ \ \ \ X[1,1]\ =\ 4;\ \ \ \ \ \ \ \ \ \ \ }
\DoxyCodeLine{\ \ \ \ }
\DoxyCodeLine{\ \ \ \ \mbox{\hyperlink{classtannic_1_1Tensor}{Tensor}}\ Y(float32,\ \{1,2\});\ Y.\mbox{\hyperlink{classtannic_1_1Tensor_a4ee063da4965ea868081841b3d967882}{initialize}}();\ \ \textcolor{comment}{//\ Explicit\ initialization\ required\ for\ now}}
\DoxyCodeLine{\ \ \ \ Y[0,0]\ =\ 4;\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{//\ but\ may\ be\ removed\ in\ the\ future.}}
\DoxyCodeLine{\ \ \ \ Y[0,1]\ =\ 6;\ \ \ \ }
\DoxyCodeLine{\ \ \ \ }
\DoxyCodeLine{\ \ \ \ Y\ =\ \mbox{\hyperlink{namespacetannic_1_1function_a9ec44a47e78a5e28b75b76573d98e06b}{log}}(X)\ +\ Y\ *\ Y\ -\/\ \mbox{\hyperlink{namespacetannic_1_1function_a1a8e818c37a5236e4481677e70ce6209}{exp}}(X)\ +\ \mbox{\hyperlink{namespacetannic_a3681656aa208b88dfc79a1806cf669b6}{matmul}}(X,\ Y.transpose());\ \textcolor{comment}{//\ assign\ expressions\ dynamically\ like\ in\ python}}
\DoxyCodeLine{\ \ \ \ std::cout\ <<\ Y;\ }
\DoxyCodeLine{\}}

\end{DoxyCode}


It will output\+:


\begin{DoxyCode}{0}
\DoxyCodeLine{Tensor([[23.2817,\ 43.2817],\ }
\DoxyCodeLine{\ \ \ \ \ \ \ \ [33.0131,\ 18.7881]]\ dtype=float32,\ shape=(2,\ 2))}

\end{DoxyCode}


Equivalent Py\+Torch code for comparison\+:


\begin{DoxyCode}{0}
\DoxyCodeLine{\textcolor{keyword}{import}\ torch}
\DoxyCodeLine{\ }
\DoxyCodeLine{X\ =\ torch.zeros((2,\ 2),\ dtype=torch.float32)}
\DoxyCodeLine{\ }
\DoxyCodeLine{X[0,\ 0:]\ =\ 1\ \ \ \ \ \ \ }
\DoxyCodeLine{X[1,\ 0]\ =\ 3}
\DoxyCodeLine{X[1,\ 1]\ =\ 4}
\DoxyCodeLine{\ }
\DoxyCodeLine{Y\ =\ torch.zeros((1,\ 2),\ dtype=torch.float32)}
\DoxyCodeLine{\ }
\DoxyCodeLine{Y[0,\ 0]\ =\ 4\ \ \ \ \ }
\DoxyCodeLine{Y[0,\ 1]\ =\ 6\ \ \ \ \ \ \ }
\DoxyCodeLine{Y\ =\ torch.log(X)\ +\ Y\ *\ Y\ -\/\ torch.exp(X)\ +\ torch.matmul(X,\ Y.t())}
\DoxyCodeLine{print(Y)\ }

\end{DoxyCode}
\hypertarget{index_autotoc_md28}{}\doxysection{\texorpdfstring{Status}{Status}}\label{index_autotoc_md28}
Note\+: Tannic is currently in an early development stage. It is functional but not fully optimized, and some features may still have bugs. The C backend API—used to extend the library—is under active development and may change significantly. The public API described in the documentation is mostly stable, with only minor breaking changes expected as the library evolves.

While the library is currently written in C++23, the arrival of C++26, is shaping up to be a monumental-\/ too significant to ignore. At some point, it may be a hard requirement for Tannic.\hypertarget{index_autotoc_md29}{}\doxysection{\texorpdfstring{Features}{Features}}\label{index_autotoc_md29}

\begin{DoxyItemize}
\item Dynamic typing\+: Flexible tensor data types that support runtime type specification, enabling features like easy tensor serialization and deserialization.
\item Broadcasting\+: Num\+Py‑style automatic shape expansion in arithmetic operations, enabling intuitive and efficient tensor computations across dimensions.
\item Advanced indexing and slicing\+: Intuitive multi-\/dimensional tensor access and manipulation.
\item Host–\+Device execution model\+: Unified support for CPU and CUDA-\/enabled GPU computation within the same codebase. While the device backend is currently developed in CUDA, the design is not tied to it and can support other backends in the future.
\item Minimal core operators\+: Only essential built-\/in operations to keep the library lightweight and extensible.
\end{DoxyItemize}\hypertarget{index_autotoc_md30}{}\doxysection{\texorpdfstring{What is comming...}{What is comming...}}\label{index_autotoc_md30}

\begin{DoxyItemize}
\item Autograd\+: Autograd is not necessary for inference, so it will be added to the library later when the runtime api is optimized and mature.
\item Graph mode\+: A constexpr graph mode will be added to the library, possibly with the arrival of C++26.
\item Quantization support\+: The library will support necessary dtypes to create quantized neural networks like bitnet.
\item Additional backends\+: Expansion beyond CUDA to support other device backends is planned. Host-\/\+Device computational model can be used as well with other hardware vendors.
\end{DoxyItemize}

\DoxyHorRuler{0}
\hypertarget{index_autotoc_md32}{}\doxysection{\texorpdfstring{Requirements}{Requirements}}\label{index_autotoc_md32}

\begin{DoxyItemize}
\item C++23 compiler\+: A compiler with C++23 support is required to build and run Tannic.
\item Open\+BLAS (optional)\+: If installed on your system, Open\+BLAS will accelerate matrix multiplication.
\item CUDA Toolkit (optional)\+: CUDA 12+ is required only if you want GPU support. If not installed, Tannic will build and run with CPU-\/only support.
\end{DoxyItemize}\hypertarget{index_autotoc_md33}{}\doxysection{\texorpdfstring{Installation}{Installation}}\label{index_autotoc_md33}
Clone the repository and include {\ttfamily tannic.\+hpp} and build it as follows\+:\hypertarget{index_autotoc_md34}{}\doxysubsection{\texorpdfstring{Debug build\+:}{Debug build:}}\label{index_autotoc_md34}
Use this for development — includes extra checks, assertions, and debug symbols for easier troubleshooting.


\begin{DoxyCode}{0}
\DoxyCodeLine{mkdir\ build\ \&\&\ cd\ build}
\DoxyCodeLine{cmake\ ..\ -\/DCMAKE\_BUILD\_TYPE=Debug}
\DoxyCodeLine{make\ -\/j\$(nproc)}
\DoxyCodeLine{ctest\ -\/-\/output-\/on-\/failure}

\end{DoxyCode}
\hypertarget{index_autotoc_md35}{}\doxysubsection{\texorpdfstring{Release build}{Release build}}\label{index_autotoc_md35}
Use this for deployment or benchmarking — builds with full compiler optimizations and without debug checks.


\begin{DoxyCode}{0}
\DoxyCodeLine{mkdir\ build\ \&\&\ cd\ build}
\DoxyCodeLine{cmake\ ..\ -\/DCMAKE\_BUILD\_TYPE=Release}
\DoxyCodeLine{make\ -\/j\$(nproc)\ }

\end{DoxyCode}
\hypertarget{index_autotoc_md36}{}\doxysubsection{\texorpdfstring{Run the example}{Run the example}}\label{index_autotoc_md36}
You can run the example provided in the main.\+cpp from the build folder\+: 
\begin{DoxyCode}{0}
\DoxyCodeLine{./main}

\end{DoxyCode}
\hypertarget{index_autotoc_md37}{}\doxysubsection{\texorpdfstring{Include Tannic in your project}{Include Tannic in your project}}\label{index_autotoc_md37}

\begin{DoxyCode}{0}
\DoxyCodeLine{\textcolor{preprocessor}{\#include\ <tannic.hpp>}}

\end{DoxyCode}
\hypertarget{index_autotoc_md38}{}\doxysubsection{\texorpdfstring{CUDA support}{CUDA support}}\label{index_autotoc_md38}
CUDA support is enabled by default if a compatible CUDA toolkit (12+) is detected during configuration. If no CUDA installation is found, Tannic will automatically fall back to a CPU‑only build. You can explicitly disable CUDA with\+:


\begin{DoxyCode}{0}
\DoxyCodeLine{cmake\ ..\ -\/DTANNIC\_ENABLE\_CUDA=OFF}

\end{DoxyCode}
\hypertarget{index_autotoc_md39}{}\doxysection{\texorpdfstring{Contributing}{Contributing}}\label{index_autotoc_md39}
Contributions to Tannic are welcome! If you\textquotesingle{}d like to report bugs, request features, or submit pull requests, please follow these guidelines\+:


\begin{DoxyItemize}
\item Fork the repository and create a new branch for your feature or bugfix.
\item Include tests and documentation for new features or bug fixes.
\item Open a pull request describing your changes and their purpose.
\end{DoxyItemize}

Tannic is licensed under the Apache License 2.\+0, a permissive open-\/source license that allows you to use, modify, and distribute the code freely—even in commercial projects.

The only thing I ask in return is proper credit to the project and its contributors. Recognition helps the project grow and motivates continued development.

By contributing, you agree that your contributions will also be licensed under Apache 2.\+0 and that proper attribution is appreciated.\hypertarget{index_autotoc_md40}{}\doxysection{\texorpdfstring{License}{License}}\label{index_autotoc_md40}
Tannic is licensed under the Apache License, Version 2.\+0. See the \mbox{[}LICENSE\mbox{]}(LICENSE) file for details. 