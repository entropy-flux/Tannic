# Tensors

## Introduction

The tensor class is the main data structure of the library. It is basically an intention of a computation and it's coordinates can be seen as a multidimensional array. Tensors in this library are not templated, while this have a minimal performance penalty, it's not something you are not already paying when you using torch or jax, the easy of use simply outweights some minimal performance gain that can be obtained from making the tensors templated. 

Having templated tensors means that a `Tensor<int>` and a `Tensor<float>` are different classes, while in tannic you only have a single `Tensor` class where dtype is explicitly passed as argument. This simplifies a lot the development of machine learning models that needs to support several dtypes. You can see what I'm talking about in this examples:

* A simple [CNN](https://github.com/entropy-flux/cnn-server-example/blob/main/models/cnn.hpp) (convolutional neural network) for mnist digit recognition.

* A simple [ViT](https://github.com/entropy-flux/vit-server-example/blob/main/model/vit.hpp) (vision transformer) for classification tasks

* A [LLaMa3](https://github.com/entropy-flux/llama3-server-example/blob/main/include/llama3.hpp) transformer implementation, working for text completion.

As you may see, those implementations are just as clean (if not cleaner) than their Python counterparts, with no template metaprogramming required. While I use neural networks as examples, this library is not limited to that domain. In fact, none of the neural network primitives you saw in the examples are implemented directly in the tensor library; instead, they exist as an [external nn library](https://github.com/entropy-flux/Tannic-NN) built on top of it. This approach lays the groundwork for building a complete ecosystem for machine learning and physics simulations, not only neural networks.


## Tensor initialization

This library provides two convenient ways to construct and initialize tensors. These range from implicit C++ brace-initializer syntax (used for readability in small examples) to explicit initialization (recommended when specifying dtype, shape, or device explicitly). Tensors also can be used uninitialized, and initialization will happen on the first use by default on host.

More initialization methods will be added soon, ranging from statistical distributions to widely used initialization patterns.


### 1. Implicit Initialization

Tensors can be directly constructed using nested brace lists. The nesting depth determines the rank (dimensionality) of the tensor, and the innermost values define the elements.

#### Example

```cpp 
    Tensor A = {1,2,3,4,5};

    Tensor B = {{
        1.0f, 2.0f, 3.0f}, 
        {4.0f, 5.0f, 6.0f}
    }; 

    Tensor C = {
        {{1.0, 2.0, 3.0}, {4.0, 5.0, 6.0}},
        {{1.0, 2.0, 3.0}, {4.0, 5.0, 6.0}}
    };

    std::cout << A << std::endl; // int tensor
    std::cout << B << std::endl; // float tensor
    std::cout << C << std::endl; // double tensor
```

**Note**: Implicit initialization infers dtype from the initializer list (e.g., int, float, double). If consistent typing or device placement is required, use explicit initialization. Be careful with this. 


#### 2. Explicit initialization

Explicit initialization is useful when you need to set dtype, shape, or device precisely. Initialization with initializer list will be more flexible here since the elements of the initializer list will be casted automatically to the dtype of the tensor. For example you can initialize a float tensor with an int initializer list without issues. 

#### Example

```cpp 
    Tensor X(bfloat16, {3}); X.initialize({1, 2, 3}); // integers will be casted to floats here automatically
                                                      // don't have to worry about the type of the initializer list.

    Tensor Y(float16, {2, 3}); Y.initialize({         // Here you are passing doubles to the tensor but they will be
        {1.2, 2.4, 3.1},                              // automatically casted to float16 type. You
        {4.3, 5.6, 6.3}                                    
    });

    Tensor Z(int8, {2, 2, 2}); Z.initialize({
        {
            {1, 2},
            {3, 4}
        },
        {
            {5, 6},
            {7, 8}
        }
    });

    std::cout << X << std::endl;
    std::cout << Y << std::endl;
    std::cout << Z << std::endl;
```

In the prior example an object of type shape was constructed implicitly when calling the tensor constructor. You can pass shape or even strides explicitly to tensor constructors when building a tensor:

```cpp
    Tensor Y(float64, Shape(2, 3)); Y.initialize({
        {1, 2, 3},
        {4, 5, 6}
    });

    std::cout << Y << std::endl;

    constexpr Shape shape(2, 3);    
    constexpr Strides strides(3, 1); // shapes and strides are constexpr friendly. 

    Tensor Z(int32, shape, strides); Z.initialize({
        {1, 2, 3},
        {4, 5, 6}
    });

    std::cout << shape << strides << std::endl; // they are also printable.
    std::cout << Z << std::endl;
```

Tensors can be allocated directly on a specified device to allow harware aceleration. Tannic have only CUDA support for now, however Host-Device compuational model is not CUDA specific and can be extended to other vendors.

#### Example

```cpp
    Tensor X(float16, {3}); X.initialize({1, 2, 3}, Device()); // defaults to 0. 
    std::cout << X << std::endl;
```

### Other initialization methods.

There are more ways to initialize tensors, but they are not documented here because their usage may change, they are from internal usage or are out of the scope of this section, like for example how tensors are built when they come serialized from network. 

### 3. Assignment

After creation, tensors support element-wise and slice assignment. If they weren't initialized before asignment they will be initialized by default on host. Unassigned elements remain uninitialized until written to.

```cpp
    Tensor X(bfloat16, {2,2});   
    X[0, range{0,-1}] = 1; 
    X[1,0] = 3;
    X[1][1] = 4; 
    std::cout << X << std::endl;


    Tensor Y(float64, {2,2}); Y.initialize(Device());  
    Y[{0,-1}] = 1; 
    std::cout << Y << std::endl; 
```

### 4. Supported dtypes.

The library provides a fixed set of supported data types. They are defined as a plain C-compatible enum, so they can be safely used in both C and C++ code. In C++, all dtypes are available under the tannic namespace. The following list of dtypes is supported. 

| Library dtype | Native C/C++ equivalent           |
| ------------- | --------------------------------- |
| `any`         | (placeholder, no fixed type)      |
| `boolean`     | `bool` (bit packed)               |
| `int8`        | `int8_t`                          |
| `int16`       | `int16_t`                         |
| `int32`       | `int32_t`                         |
| `int64`       | `int64_t`                         |
| `float16`     | compiler-specific                 |
| `bfloat16`    | compiler-specific                 |
| `float32`     | `float`                           |
| `float64`     | `double`                          |
| `complex64`   | `std::complex<float>`             |
| `complex128`  | `std::complex<double>`            | 

Expect to have `int4` and `complex32` dtypes support soon. 

### 5. Copy semantics

The Tensor class is shallow copiable, this means that is cheap to copy tensors, and copied tensors will reference to the same underlying storage, for example:


#### Example

```cpp
Tensor X = {1, 2, 3, 4, 5};
Tensor Y = X;         // Cheap copy,  
Tensor Z = Y[{1, 3}]; // Cheap view
Z[{0, -1}] = 5;       // Will modify X tensor.
std::cout << X << std::endl; 
```

Will print a modified tensor.

```
Tensor([1, 5, 5, 4, 5], dtype=int32, shape=(5))
```